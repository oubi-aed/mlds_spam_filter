{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0a204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Setze den Suchpfad für NLTK-Daten\n",
    "nltk.data.path.append(r'C:\\Users\\alexa\\AppData\\Roaming\\nltk_data')\n",
    "\n",
    "# Stelle sicher, dass punkt installiert ist\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', download_dir=r'C:\\Users\\alexa\\AppData\\Roaming\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d810534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_emails_from_folder(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, encoding='latin-1') as f:  # <- latin-1 statt utf-8!\n",
    "                emails.append(f.read())\n",
    "    return emails\n",
    "\n",
    "# Beispiel: Lade alle Spam-Mails\n",
    "spam_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\spam')\n",
    "ham_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\ham')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acadc7",
   "metadata": {},
   "source": [
    "split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59efa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Labels: 1 für Spam, 0 für Ham\n",
    "labels_spam = [1] * len(preprocessed_spam)\n",
    "labels_ham = [0] * len(preprocessed_ham)\n",
    "\n",
    "# 2. Alle Daten kombinieren\n",
    "all_emails = preprocessed_spam + preprocessed_ham\n",
    "all_labels = labels_spam + labels_ham\n",
    "\n",
    "\n",
    "\n",
    "# 3. Aufteilen in Trainings- und Testdaten (z. B. 80 % Training, 20 % Test)\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    all_emails, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05014b",
   "metadata": {},
   "source": [
    "formate the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0ca2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " mv 1 00001.bfc8d64d12b325ff385cca8d07b84288\n",
      "mv 10 00010.7f5fb525755c45eb78efc18d7c9ea5aa\n",
      "mv 100 00100.c60d1c697136b07c947fa180ba3e0441\n",
      "mv 101 00101.2dfd7ee79ae439b8d9c38e783a137efa\n",
      "mv 102 00102.2e3969075728dde7a328e05d19b35976\n",
      "mv 103 00103.8c39bfed2079f865e9dfb75f4416a468\n",
      "mv 104 00104.886f4a22362f4d3528c3e675878f17f7\n",
      "mv 105 00105.9790e1c57fcbf7885b7cd1719fb4681b\n",
      "mv 106 00106.fa6df8609cebb6f0f37aec3f70aa5b9a\n",
      "mv 107 00107.f1d4194b57840ea6587b9a73ed88e075\n",
      "mv 108 00108.4506c2ef846b80b9a7beb90315b227\n",
      "\n",
      "Vorverarbeitet:\n",
      " mv number numberbfcnumberdnumberdnumberbnumberffnumberccanumberdnumberbnumb mv number numbernumberfnumberfbnumbercnumberebnumberefcnumberdnumbercnumbereanumberaa mv number numbercnumberdnumbercnumberbnumbercnumberfanumberbanumberenumb mv number numbernumberdfdnumbereenumberaenumberbnumberdnumbercnumberenumberanumberefa mv number numbernumberenumberddenumberanumberenumberdnumberbnumb mv number numbernumbercnumberbfednumberfnumberenumberdfbnumberfnumberanumb mv number numbernumberfnumberanumberfnu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Nur beim ersten Mal nötig:\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def load_emails_from_folder(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                emails.append(f.read())\n",
    "    return emails\n",
    "\n",
    "def preprocess_email(text):\n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "\n",
    "    # URLs ersetzen\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text)\n",
    "\n",
    "    # Zahlen ersetzen\n",
    "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "\n",
    "    # Satzzeichen entfernen\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenisierung (für Stemming nötig)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# === Beispielverwendung ===\n",
    "spam_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\spam')\n",
    "ham_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\ham')\n",
    "\n",
    "# Vorverarbeitung anwenden\n",
    "preprocessed_spam = [preprocess_email(email) for email in spam_emails]\n",
    "preprocessed_ham = [preprocess_email(email) for email in ham_emails]\n",
    "\n",
    "# Optional: Beispiel ausgeben\n",
    "print(\"Original:\\n\", spam_emails[0][:500])\n",
    "print(\"\\nVorverarbeitet:\\n\", preprocessed_spam[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd77ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vokabular (Top 20): ['number', 'the', 'numbernumbernumb', 'from', 'to', 'for', 'with', 'by', 'receiv', 'numbernumbernumbernumb', 'of', 'and', 'a', 'id', 'sep', 'url', 'in', 'esmtp', 'localhost', 'numbernumb']\n",
      "Binär-Vektor (erste Spam-Mail): [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Zähl-Vektor (erste Spam-Mail): [500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Tokenisieren (die E-Mails wurden bereits vorverarbeitet → einfach splitten)\n",
    "tokenized_spam = [email.split() for email in preprocessed_spam]\n",
    "tokenized_ham = [email.split() for email in preprocessed_ham]\n",
    "all_tokenized = tokenized_spam + tokenized_ham\n",
    "\n",
    "# 2. Vokabular erstellen (z. B. Top 1000 häufigste Wörter)\n",
    "def build_vocabulary(tokenized_emails, vocab_size=None):\n",
    "    all_tokens = []\n",
    "    for tokens in tokenized_emails:\n",
    "        all_tokens.extend(tokens)\n",
    "    word_counts = Counter(all_tokens)\n",
    "    most_common = word_counts.most_common(vocab_size)\n",
    "    vocabulary = [word for word, _ in most_common]\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(all_tokenized, vocab_size=1000)\n",
    "\n",
    "# 3. Funktion zur Vektorerstellung\n",
    "def email_to_vector(tokens, vocabulary, binary=True):\n",
    "    token_counts = Counter(tokens)\n",
    "    vector = []\n",
    "    for word in vocabulary:\n",
    "        if binary:\n",
    "            vector.append(1 if word in token_counts else 0)\n",
    "        else:\n",
    "            vector.append(token_counts[word])\n",
    "    return vector\n",
    "\n",
    "# 4. Vektorisieren aller E-Mails\n",
    "binary_vectors_spam = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_spam]\n",
    "binary_vectors_ham = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_ham]\n",
    "\n",
    "count_vectors_spam = [email_to_vector(tokens, vocabulary, binary=False) for tokens in tokenized_spam]\n",
    "count_vectors_ham = [email_to_vector(tokens, vocabulary, binary=False) for tokens in tokenized_ham]\n",
    "\n",
    "# 5. Beispielausgabe\n",
    "print(\"Vokabular (Top 20):\", vocabulary[:20])\n",
    "print(\"Binär-Vektor (erste Spam-Mail):\", binary_vectors_spam[0][:20])\n",
    "print(\"Zähl-Vektor (erste Spam-Mail):\", count_vectors_spam[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9afb3",
   "metadata": {},
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Tokenisiere die Trainings- und Testdaten\n",
    "tokenized_train = [email.split() for email in X_train_texts]\n",
    "tokenized_test = [email.split() for email in X_test_texts]\n",
    "\n",
    "# Wandle sie in Vektoren um (nutze das gleiche Vokabular wie oben)\n",
    "X_train_vectors = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_train]\n",
    "X_test_vectors = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_test]\n",
    "\n",
    "# Klassifikatoren\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(C=1, max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Alle trainieren und evaluieren\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_vectors, y_train)\n",
    "    y_pred = clf.predict(X_test_vectors)\n",
    "    \n",
    "    print(f\"\\n📌 {name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
