{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64debb0c",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d810534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_emails_from_folder(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, encoding='latin-1') as f: \n",
    "                emails.append(f.read())\n",
    "    return emails\n",
    "\n",
    "# load the data sets\n",
    "spam_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\spam')\n",
    "ham_emails = load_emails_from_folder(r'C:\\Users\\alexa\\Documents\\4. Semester Mechatronik-2025\\Machine_Learning_und_Data_Science\\mlds_spam_filter\\data\\ham')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acadc7",
   "metadata": {},
   "source": [
    "formate the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e59efa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " mv 1 00001.bfc8d64d12b325ff385cca8d07b84288\n",
      "mv 10 00010.7f5fb525755c45eb78efc18d7c9ea5aa\n",
      "mv 100 00100.c60d1c697136b07c947fa180ba3e0441\n",
      "mv 101 00101.2dfd7ee79ae439b8d9c38e783a137efa\n",
      "mv 102 00102.2e3969075728dde7a328e05d19b35976\n",
      "mv 103 00103.8c39bfed2079f865e9dfb75f4416a468\n",
      "mv 104 00104.886f4a22362f4d3528c3e675878f17f7\n",
      "mv 105 00105.9790e1c57fcbf7885b7cd1719fb4681b\n",
      "mv 106 00106.fa6df8609cebb6f0f37aec3f70aa5b9a\n",
      "mv 107 00107.f1d4194b57840ea6587b9a73ed88e075\n",
      "mv 108 00108.4506c2ef846b80b9a7beb90315b227\n",
      "\n",
      "Preprocessed:\n",
      " mv 1 00001bfc8d64d12b325ff385cca8d07b84288 mv 10 000107f5fb525755c45eb78efc18d7c9ea5aa mv 100 00100c60d1c697136b07c947fa180ba3e0441 mv 101 001012dfd7ee79ae439b8d9c38e783a137efa mv 102 001022e3969075728dde7a328e05d19b35976 mv 103 001038c39bfed2079f865e9dfb75f4416a468 mv 104 00104886f4a22362f4d3528c3e675878f17f7 mv 105 001059790e1c57fcbf7885b7cd1719fb4681b mv 106 00106fa6df8609cebb6f0f37aec3f70aa5b9a mv 107 00107f1d4194b57840ea6587b9a73ed88e075 mv 108 001084506c2ef846b80b9a7beb90315b22701 mv 109 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK components\n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Load all emails from a folder\n",
    "def load_emails_from_folder(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                emails.append(f.read())\n",
    "    return emails\n",
    "\n",
    "\n",
    "# Preprocess a single email by cleaning and simplifying its contents\n",
    "def preprocess_email(text):\n",
    "\n",
    "    # Remove email header (everything before the first empty line)\n",
    "    if \"\\n\\n\" in text:\n",
    "        text = text.split(\"\\n\\n\", 1)[1]\n",
    "        \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs with placeholder\n",
    "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', 'URL', text)\n",
    "\n",
    "    # Replace numbers with placeholder\n",
    "    text = re.sub(r'\\\\d+', 'NUMBER', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text for stemming\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "spam_emails = load_emails_from_folder(\n",
    "    r'C:\\\\Users\\\\alexa\\\\Documents\\\\4. Semester Mechatronik-2025\\\\Machine_Learning_und_Data_Science\\\\mlds_spam_filter\\\\data\\\\spam'\n",
    ")\n",
    "ham_emails = load_emails_from_folder(\n",
    "    r'C:\\\\Users\\\\alexa\\\\Documents\\\\4. Semester Mechatronik-2025\\\\Machine_Learning_und_Data_Science\\\\mlds_spam_filter\\\\data\\\\ham'\n",
    ")\n",
    "\n",
    "# Apply preprocessing to each email\n",
    "preprocessed_spam = [preprocess_email(email) for email in spam_emails]\n",
    "preprocessed_ham = [preprocess_email(email) for email in ham_emails]\n",
    "\n",
    "# Optional: Print example of original and preprocessed email\n",
    "print(\"Original:\\n\", spam_emails[0][:500])\n",
    "print(\"\\nPreprocessed:\\n\", preprocessed_spam[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05014b",
   "metadata": {},
   "source": [
    "split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f0ca2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Labels: 1 for Spam, 0 for Ham\n",
    "labels_spam = [1] * len(preprocessed_spam)\n",
    "labels_ham = [0] * len(preprocessed_ham)\n",
    "\n",
    "# 2. Combine all data\n",
    "all_emails = preprocessed_spam + preprocessed_ham\n",
    "all_labels = labels_spam + labels_ham\n",
    "\n",
    "# 3. Split into training and test data (e.g., 80% training, 20% testing)\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    all_emails,              # Input data (emails)\n",
    "    all_labels,              # Corresponding labels\n",
    "    test_size=0.2,           # 20% of the data for testing\n",
    "    random_state=42,         # Ensure reproducibility\n",
    "    stratify=all_labels      # Maintain label distribution in both sets\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072677fb",
   "metadata": {},
   "source": [
    "convert each email into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dd77ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (Top 20): ['the', 'to', 'a', 'and', 'of', 'in', 'td', 'for', 'is', 'you', 'it', 'that', 'i', 'thi', 'tr', 'on', 'your', 'with', 'tabl', 'be']\n",
      "Binary vector (first spam email): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Count vector (first spam email): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Tokenize (emails have already been preprocessed â†’ simple split is enough)\n",
    "tokenized_spam = [email.split() for email in preprocessed_spam]\n",
    "tokenized_ham = [email.split() for email in preprocessed_ham]\n",
    "all_tokenized = tokenized_spam + tokenized_ham\n",
    "\n",
    "# 2. Build vocabulary (e.g., top 1000 most frequent words)\n",
    "def build_vocabulary(tokenized_emails, vocab_size=None):\n",
    "    all_tokens = []\n",
    "    for tokens in tokenized_emails:\n",
    "        all_tokens.extend(tokens)  # Flatten list of token lists into a single list\n",
    "    word_counts = Counter(all_tokens)  # Count word frequencies\n",
    "    most_common = word_counts.most_common(vocab_size)  # Get top N words\n",
    "    vocabulary = [word for word, _ in most_common]  # Extract just the words\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(all_tokenized, vocab_size=1000)\n",
    "\n",
    "# 3. Function to create feature vector from tokens\n",
    "def email_to_vector(tokens, vocabulary, binary=True):\n",
    "    token_counts = Counter(tokens)  # Count tokens in the email\n",
    "    vector = []\n",
    "    for word in vocabulary:\n",
    "        if binary:\n",
    "            vector.append(1 if word in token_counts else 0)  # 1 if word is present\n",
    "        else:\n",
    "            vector.append(token_counts[word])  # Count how many times the word appears\n",
    "    return vector\n",
    "\n",
    "# 4. Vectorize all emails (binary and count-based)\n",
    "binary_vectors_spam = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_spam]\n",
    "binary_vectors_ham = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_ham]\n",
    "\n",
    "count_vectors_spam = [email_to_vector(tokens, vocabulary, binary=False) for tokens in tokenized_spam]\n",
    "count_vectors_ham = [email_to_vector(tokens, vocabulary, binary=False) for tokens in tokenized_ham]\n",
    "\n",
    "# 5. Example output\n",
    "print(\"Vocabulary (Top 20):\", vocabulary[:20])\n",
    "print(\"Binary vector (first spam email):\", binary_vectors_spam[0][:20])\n",
    "print(\"Count vector (first spam email):\", count_vectors_spam[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9afb3",
   "metadata": {},
   "source": [
    "learn the model and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6e8a74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Naive Bayes:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      0.98      0.98       560\n",
      "        Spam       0.95      0.94      0.95       201\n",
      "\n",
      "    accuracy                           0.97       761\n",
      "   macro avg       0.97      0.96      0.96       761\n",
      "weighted avg       0.97      0.97      0.97       761\n",
      "\n",
      "Confusion Matrix:\n",
      " [[551   9]\n",
      " [ 12 189]]\n",
      "\n",
      "ðŸ“Œ Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.99      0.99       560\n",
      "        Spam       0.97      0.97      0.97       201\n",
      "\n",
      "    accuracy                           0.98       761\n",
      "   macro avg       0.98      0.98      0.98       761\n",
      "weighted avg       0.98      0.98      0.98       761\n",
      "\n",
      "Confusion Matrix:\n",
      " [[555   5]\n",
      " [  7 194]]\n",
      "\n",
      "ðŸ“Œ Linear SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.99      0.99       560\n",
      "        Spam       0.98      0.98      0.98       201\n",
      "\n",
      "    accuracy                           0.99       761\n",
      "   macro avg       0.99      0.99      0.99       761\n",
      "weighted avg       0.99      0.99      0.99       761\n",
      "\n",
      "Confusion Matrix:\n",
      " [[556   4]\n",
      " [  4 197]]\n",
      "\n",
      "ðŸ“Œ Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.99      0.99       560\n",
      "        Spam       0.98      0.97      0.98       201\n",
      "\n",
      "    accuracy                           0.99       761\n",
      "   macro avg       0.99      0.98      0.98       761\n",
      "weighted avg       0.99      0.99      0.99       761\n",
      "\n",
      "Confusion Matrix:\n",
      " [[557   3]\n",
      " [  6 195]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Tokenize the training and test data\n",
    "tokenized_train = [email.split() for email in X_train_texts]\n",
    "tokenized_test = [email.split() for email in X_test_texts]\n",
    "\n",
    "# Convert emails to vectors (using the same vocabulary as before)\n",
    "X_train_vectors = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_train]\n",
    "X_test_vectors = [email_to_vector(tokens, vocabulary, binary=True) for tokens in tokenized_test]\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(C=0.1, max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_vectors, y_train)               # Train the model\n",
    "    y_pred = clf.predict(X_test_vectors)            # Predict on the test set\n",
    "    \n",
    "    print(f\"\\nðŸ“Œ {name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Ham\", \"Spam\"]))  # Print precision, recall, F1-score\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))              # Show confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
